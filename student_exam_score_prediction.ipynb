{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbe2feb",
   "metadata": {},
   "source": [
    "# Student Exam Score Prediction\n",
    "\n",
    "* Sandikha Rahardi (Kuldii Project)\n",
    "* https://kuldiiproject.com\n",
    "\n",
    "This notebook builds a machine learning solution to predict students' exam scores (math, reading, writing) based on their study-related features. The workflow includes EDA, preprocessing, model training, hyperparameter tuning, model comparison, and a Gradio interface for interactive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc965d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e37bd",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Download the dataset from [Kaggle](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams/data) and place the CSV file in this project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using Kaggle API if not present\n",
    "csv_file = 'StudentsPerformance.csv'\n",
    "kaggle_dataset = 'spscientist/students-performance-in-exams'\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    # Install kaggle if needed\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        !{sys.executable} -m pip install kaggle\n",
    "    # Set up Kaggle API credentials\n",
    "    kaggle_json_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        shutil.copy('kaggle.json', kaggle_json_path)\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "    # Download dataset\n",
    "    !kaggle datasets download -d {kaggle_dataset} --unzip -p datasets\n",
    "    print(f\"Extracted {csv_file}\")\n",
    "else:\n",
    "    print(f\"{csv_file} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/'+csv_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c195e",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the dataset: view the first and last rows, shape, info, summary statistics, and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print('First 5 rows:')\n",
    "display(df.head())\n",
    "print('Last 5 rows:')\n",
    "display(df.tail())\n",
    "print(f'Shape: {df.shape}')\n",
    "print('\\nInfo:')\n",
    "df.info()\n",
    "print('\\nDescribe:')\n",
    "display(df.describe())\n",
    "print('\\nMissing values:')\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "categorical_features = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "\n",
    "for col in categorical_features:\n",
    "    print(f'\\n{col} unique values:')\n",
    "    print(df[col].value_counts())\n",
    "    sns.countplot(data=df, x=col)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c638fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variables: histograms and boxplots\n",
    "score_columns = ['math score', 'reading score', 'writing score']\n",
    "\n",
    "for col in score_columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.histplot(df[col], kde=True, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title(f'Histogram of {col}')\n",
    "    sns.boxplot(x=df[col], ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title(f'Boxplot of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of target variables\n",
    "sns.pairplot(df[score_columns], kind='reg', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Target Variables', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = df[score_columns].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Target Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a109dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scores vs categorical variables: grouped averages and boxplots\n",
    "for cat in categorical_features:\n",
    "    print(f'\\nAverage scores grouped by {cat}:')\n",
    "    display(df.groupby(cat)[score_columns].mean())\n",
    "    df_grouped = df.groupby(cat)[score_columns].mean().reset_index()\n",
    "    df_grouped_melt = df_grouped.melt(id_vars=cat, var_name='Score Type', value_name='Average Score')\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(data=df_grouped_melt, x=cat, y='Average Score', hue='Score Type')\n",
    "    plt.title(f'Average Scores by {cat}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    for score in score_columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.boxplot(data=df, x=cat, y=score)\n",
    "        plt.title(f'{score} by {cat}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31637e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using z-score\n",
    "from scipy.stats import zscore\n",
    "z_scores = np.abs(zscore(df[score_columns]))\n",
    "outliers = (z_scores > 3).any(axis=1)\n",
    "print(f'Number of potential outliers: {outliers.sum()}')\n",
    "display(df[outliers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec917db6",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Encode categorical variables, separate features and targets, and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa38e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df[categorical_features]\n",
    "y = df[score_columns]\n",
    "\n",
    "# OneHotEncode categorical features\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = ohe.fit_transform(X)\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=ohe.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Optionally scale features (not strictly necessary for tree-based models)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded_df)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape}, Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6148e",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train and compare LinearRegression, RandomForestRegressor, and GradientBoostingRegressor using MultiOutputRegressor for multi-target regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ab4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': MultiOutputRegressor(LinearRegression()),\n",
    "    'RandomForestRegressor': MultiOutputRegressor(RandomForestRegressor(random_state=42)),\n",
    "    'GradientBoostingRegressor': MultiOutputRegressor(GradientBoostingRegressor(random_state=42))\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    results[name] = evaluate_model(model, X_test, y_test)\n",
    "    print(f'{name} evaluation:', results[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770c80d",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Tune RandomForestRegressor and GradientBoostingRegressor using GridSearchCV. Evaluate using RMSE, MAE, and R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for RandomForestRegressor\n",
    "rf_param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [None, 5, 10],\n",
    "    'estimator__min_samples_split': [2, 5]\n",
    "}\n",
    "rf_grid = GridSearchCV(MultiOutputRegressor(RandomForestRegressor(random_state=42)), rf_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print('Best RandomForestRegressor params:', rf_grid.best_params_)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_eval = evaluate_model(rf_best, X_test, y_test)\n",
    "print('RandomForestRegressor (tuned) evaluation:', rf_eval)\n",
    "\n",
    "# Hyperparameter tuning for GradientBoostingRegressor\n",
    "gb_param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__learning_rate': [0.05, 0.1],\n",
    "    'estimator__max_depth': [3, 5]\n",
    "}\n",
    "gb_grid = GridSearchCV(MultiOutputRegressor(GradientBoostingRegressor(random_state=42)), gb_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "print('Best GradientBoostingRegressor params:', gb_grid.best_params_)\n",
    "gb_best = gb_grid.best_estimator_\n",
    "gb_eval = evaluate_model(gb_best, X_test, y_test)\n",
    "print('GradientBoostingRegressor (tuned) evaluation:', gb_eval)\n",
    "\n",
    "# Update results\n",
    "tuned_results = results.copy()\n",
    "tuned_results['RandomForestRegressor (tuned)'] = rf_eval\n",
    "tuned_results['GradientBoostingRegressor (tuned)'] = gb_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ae591",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Compare the performance of all models using RMSE, MAE, and R² metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52643a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "comparison_df = pd.DataFrame(tuned_results).T\n",
    "comparison_df = comparison_df[['RMSE', 'MAE', 'R2']]\n",
    "display(comparison_df.sort_values('RMSE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18f560",
   "metadata": {},
   "source": [
    "## 8. Select Best Model\n",
    "\n",
    "Refit the best-performing model on the entire training set for final use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd721620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and refit the best model\n",
    "best_model_name = comparison_df['RMSE'].idxmin()\n",
    "print(f'Best model: {best_model_name}')\n",
    "\n",
    "if best_model_name == 'RandomForestRegressor (tuned)':\n",
    "    final_model = rf_best\n",
    "elif best_model_name == 'GradientBoostingRegressor (tuned)':\n",
    "    final_model = gb_best\n",
    "else:\n",
    "    final_model = models.get(best_model_name, None)\n",
    "\n",
    "# Refit on all training data\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03381e39",
   "metadata": {},
   "source": [
    "## 9. Feature Importance\n",
    "\n",
    "Plot feature importances for tree-based models (RandomForestRegressor, GradientBoostingRegressor) if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances for tree-based models\n",
    "if hasattr(final_model.estimators_[0], 'feature_importances_'):\n",
    "    importances = np.mean([est.feature_importances_ for est in final_model.estimators_], axis=0)\n",
    "    feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feat_imp_df = feat_imp_df.sort_values('Importance', ascending=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=feat_imp_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Feature importances not available for this model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e82ca",
   "metadata": {},
   "source": [
    "## 10. Gradio Interface\n",
    "\n",
    "Create a Gradio UI to select a model and input features for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5641172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface for predictions\n",
    "# Prepare model dictionary for Gradio\n",
    "model_dict = {\n",
    "    'LinearRegression': models['LinearRegression'],\n",
    "    'RandomForestRegressor': rf_best,\n",
    "    'GradientBoostingRegressor': gb_best\n",
    "}\n",
    "\n",
    "def preprocess_input(gender, race, parental, lunch, prep):\n",
    "    input_df = pd.DataFrame({\n",
    "        'gender': [gender],\n",
    "        'race/ethnicity': [race],\n",
    "        'parental level of education': [parental],\n",
    "        'lunch': [lunch],\n",
    "        'test preparation course': [prep]\n",
    "    })\n",
    "    input_encoded = ohe.transform(input_df)\n",
    "    input_scaled = scaler.transform(input_encoded)\n",
    "    return input_scaled\n",
    "\n",
    "def predict_scores(model_name, gender, race, parental, lunch, prep):\n",
    "    model = model_dict[model_name]\n",
    "    X_input = preprocess_input(gender, race, parental, lunch, prep)\n",
    "    pred = model.predict(X_input)[0]\n",
    "    return round(pred[0], 2), round(pred[1], 2), round(pred[2], 2)\n",
    "\n",
    "# Get dropdown options from data\n",
    "options = {col: sorted(df[col].unique()) for col in categorical_features}\n",
    "\n",
    "gradio_inputs = [\n",
    "    gr.Dropdown(choices=list(model_dict.keys()), label='Model'),\n",
    "    gr.Dropdown(choices=options['gender'], label='Gender'),\n",
    "    gr.Dropdown(choices=options['race/ethnicity'], label='Race/Ethnicity'),\n",
    "    gr.Dropdown(choices=options['parental level of education'], label='Parental Level of Education'),\n",
    "    gr.Dropdown(choices=options['lunch'], label='Lunch'),\n",
    "    gr.Dropdown(choices=options['test preparation course'], label='Test Preparation Course')\n",
    "]\n",
    "\n",
    "gradio_outputs = [\n",
    "    gr.Number(label='Predicted Math Score'),\n",
    "    gr.Number(label='Predicted Reading Score'),\n",
    "    gr.Number(label='Predicted Writing Score')\n",
    "]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict_scores,\n",
    "    inputs=gradio_inputs,\n",
    "    outputs=gradio_outputs,\n",
    "    title=\"Student Exam Score Prediction\",\n",
    "    description=\"Select a model and input student features to predict math, reading, and writing scores.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
